"""

This contains all the functions to calculate the g-index for an intelligence system.

Glossary : 

`intelligent_system (IS)` : the intelligent system (IS) generates a `skill program (S)` to do `task (T)`
`task (T)`: a natural language prompt that forms an instruction to the machine what to do 
`skill program (S)`: a program or DAG generated by an `IS` to perform task `T`
`curricula (C)`: data that the `IS` uses to train itself is denoted by `C` or Curricula, in our case all data in the train_array
`curriculum item (C_n)` : `C_n` indicates a subset of the curriculum that is specific to a certain template/domain
`domain (D)` : a related set of tasks/curricula. currently each template in our system can be it's own domain, but later we can group similar ones together into similar domains.
`divergence_score (DS)` : distance between skill program created by a narrow rule-based generator, and a general-purpose `IS`
`performance_theta (θ)` : performance of the `IS` on a task (`1 - DS`)
`domain_distance_score` : fractional divergence/distance between any two given templates/domains/skill programs. `DS` can be used here too, if they are the same.
`generalization_difficulty (GD)` : `domain_distance_score` between task domain & curricula domain (or training task and given test flow)
`P(C_n)` : probability of finding curricula of domain type `C_n` in curricula. or (samples_of_template_C_n/total_number_C_samples)
`priors (P)` : data which is built into the system _before_ training (are you fine-tuning something already fine-tuned?). This is negligible (~0) for most of current models.
`experience (E)` : exposure of the `IS` to `Curricula` (tentative : compute * curricula?)
`task_contribution` : contribution of performance on one `task (T)` to the overall `g_index`
this is equal to, for any domain `C_n` in total n domains in curricula `C` -
= `θ * Σ (P(C_n) * ( (GD(T,C_n)/(P + E(C_n)) )))`
`tasks_scope` or `task_domains`: all the tasks/domains in the test array
`g_index` : average of task_contribution across all tasks_scope, a measure of intelligence :
`avg_across_T(θ * Σ (P(C_n) * ( (GD(T_i,C_n)/(P + E(C_n)) ))))`

"""
import itertools
import json
import os
import statistics
from collections import defaultdict
from dataclasses import asdict, dataclass, field
from typing import Dict, List, Union

import numpy as np
import pandas as pd
import seaborn as sns

from node_utils import node_divergence
from utils import (AVAILABLE_TEMPLATES, TEMPLATE_DETAILS, Dataset,
                   DatasetDetails, domain_distance)


@dataclass(frozen=True)
class ExperimentIndices:
    GD: Dict[str, Dict[str,float]] 
    PC: Dict[str, float]
    PTheta: Dict[str, float]
    TaskDomains: List[str] = field(default_factory=list)
    CurriculaDomains: List[str] = field(default_factory=list)
    P: float = 0.0
    E: float = 0.0
    GIndex: float = 0.0

class Benchmark:
    def __init__(self, experiment):
        """
        initialize by passing in the experiment JSON.
        """
        self.experiment = experiment

    def calculate_avg_performance(self):
        """
        This contains the logic to calculate the benchmark end to end, by taking one
        experiment.json as input.
        `avg_across_T(θ)`
        """
        task_domains_scope = self.get_task_domains()
        task_contributions = []
        for task in task_domains_scope:
            task_contribution = self.get_performance_theta(task)
            task_contributions.append(task_contribution)
        avg_performance = statistics.mean(task_contributions)
        return round(avg_performance, 4)

    def get_curricula_domains(self) -> Dict:
        """
        This takes in a list of templates in the training array and returns
        a set of curriculum domains. In initial versions this is just a list of all templates, with 
        number of samples per template.
        """
        return self.experiment["train_set"]["data"]

    def probability_of_curricula(self, curriculum: str, curricula_domains: Dict) -> float:
        """
        What is the probability of the given curriculum domain occurring in 
        the complete curricula. Initially, this is just (number_of_samples_of_templateN/total_no_of_samples)
        """
        total = 0

        for curriculum_item in curricula_domains:
            total += curriculum_item["num_samples"]
        probability = curriculum["num_samples"]/total
        return probability

    def get_task_domains(self) -> Dict:
        """
        Get a list of all tasks, i.e. scope of tasks
        """
        return self.experiment["test_set"]["data"]

    def get_task_contribution(self, task: Dict) -> float:
        """
        This details the contribution of each task to the total g_index.
        this is equal to, for any domain `C_n` in total n domains in curricula `C` -
        = `θ * Σ (P(C_n) * ( (GD(T,C_n)/(P + E(C_n)) )))`
        """
        curricula_domains = self.get_curricula_domains()
        performance_theta = self.get_performance_theta(task)
        curriculum_items = []
        for curriculum in curricula_domains:
            # generalization difficulty
            GD = self.get_generalization_difficulty(task, curriculum)
            # probability of curriculum occurring in curriculum
            PC = self.probability_of_curricula(curriculum, curricula_domains)

            # get priors
            P = self.get_priors()
            E = self.get_experience()
            # curriculum_items.append(GD*(PC))

            curriculum_items.append(GD*(PC/(P+E)))

        contribution = performance_theta * sum(curriculum_items)

        return contribution

    def get_priors(self)-> float:
        """
        `priors (P)` : data which is built into the system _before_ training (are you fine-tuning something already fine-tuned?). This is negligible (~0) for most of current models.
        """
        return 0.0001
    
    def get_task_compute(self) ->float:
        """
        compute exposure to a curricula
        """
        
        return self.experiment["train_cost"]["compute"] + self.experiment["test_cost"]["compute"]

    def get_experience(self) ->float:
        """
        `experience (E)` : exposure of the `IS` to `Curricula` (tentative : compute * curricula?)
        """
        training_compute = self.get_task_compute()
        return np.log(training_compute)

    def get_domain_details(self, domain: Dict ) -> Dict:
        """
        In this case, this returns the template details for a given template.
        Output : 
        {
                "dag_length" : {
                     "inflated": 937.0,
                     "deflated": 429.2,
                }
        """
        if domain["name"] in AVAILABLE_TEMPLATES:
            extracted_details = {
                "dag_length" : TEMPLATE_DETAILS.get(domain["name"]),
            }

            return extracted_details
        else :
            return {
                "dag_length" : {
                     "inflated": 0,
                     "deflated": 0,
                }
            }

    def get_performance_theta(self, task: Dict) -> float:
        """
        This gets the performance theta on the task.
        """
        performance_per_template = self.experiment["performance"]["templates"]
        selected_task = next(
            (x for x in performance_per_template if x["name"] == task["name"]), None)
        domain_details = self.get_domain_details(selected_task)
        performance = 1 - selected_task["divergence"]

        return performance * domain_details["dag_length"]["deflated"]

    def get_generalization_difficulty(self, task_domain: Dict, curriculum_domain: Dict) -> float:
        """
        returns the exponential generalization difficulty
        (domain_distance_score * 10)^e
        """
        score = self.domain_distance_score(task_domain, curriculum_domain)
        return (score * 10) ** 2.71828

    def domain_distance_score(self, task_domain: Dict , curriculum_domain: Dict) -> Dict:
        """
        This calculates the domain distance scores, or generalization difficulty between
        a task and the curricula.
        """
        # return random.uniform(0, 1)
        total_samples = 2
        task_dataset_details = DatasetDetails(total=total_samples, data=[{"name": task_domain["name"], "num_samples":total_samples}])
        curriculum_dataset_details = DatasetDetails(total=total_samples, data=[{"name": curriculum_domain["name"], "num_samples":total_samples}])
        domain_distance_score = domain_distance(Dataset.from_details(
            task_dataset_details), Dataset.from_details(curriculum_dataset_details))
        return domain_distance_score

    def _drop_duplicate_tuples(self,l):
        """
        Drop the tuples (b,a) if (a,b) exists already
        """
        uniq = []
        for i in l:
            if not (i in uniq or tuple([i[1], i[0]]) in uniq):
                uniq.append(i)
        return uniq

    def GetExperimentIndices(self,return_dict: bool =False) -> Union[ExperimentIndices,Dict]:
        TaskDomains = self.get_task_domains()
        CurriculaDomains = self.get_curricula_domains()
        GD = { task.get('name'): {curricula.get('name'): self.get_generalization_difficulty(task,curricula) for curricula in CurriculaDomains} for task in TaskDomains }
        PC = { curricula.get('name'):self.probability_of_curricula(curricula,CurriculaDomains) for curricula in CurriculaDomains }
        P = self.get_priors()
        E = self.get_experience()
        Ptheta = { task.get('name'):self.get_performance_theta(task) for task in TaskDomains }
        #Finding out Contribution for every task
        TaskContributions = defaultdict()
        for task in TaskDomains:
            TaskContributions[task.get('name')] = Ptheta.get(task.get('name')) * sum( [ GD.get(task.get('name')).get(curricula.get('name')) * (PC.get(curricula.get('name'))/ (P+E)) for curricula in CurriculaDomains])


        TaskContributions = dict(TaskContributions)
        GIndex = round(sum(list(TaskContributions.values())),3)
        exp_indices = ExperimentIndices(GD=GD,PC=PC,PTheta=Ptheta,TaskDomains=TaskDomains,
                            CurriculaDomains=CurriculaDomains,
                            P=P,E=E,GIndex=GIndex)
        if return_dict:
            return asdict(exp_indices)
        return exp_indices


    def generate_confusion_matrix(self):
        template_pair_half_grid = self._drop_duplicate_tuples(list(itertools.product(AVAILABLE_TEMPLATES,AVAILABLE_TEMPLATES)))
        tp_dict = {f: json.load(open(os.path.join(os.getcwd(),'templates',f)))['flow'] for f in AVAILABLE_TEMPLATES }
        df = pd.DataFrame(columns=AVAILABLE_TEMPLATES,index=AVAILABLE_TEMPLATES)
        
        for ent in template_pair_half_grid:
            tp0,tp1 = ent[0], ent[1]
            if df[tp1][tp0] != -1:
                df[tp0][tp1] = df[tp1][tp0]
            else:
                df[tp0][tp1] = node_divergence(tp_dict.get(tp0,[]),tp_dict.get(tp1,[]))

            if df[tp1][tp0] == -1.0 and df[tp0][tp1] != -1.0:
                df[tp1][tp0] = df[tp0][tp1]

        sns.heatmap(df,vmin=0.0,vmax=1.0,annot=True)
