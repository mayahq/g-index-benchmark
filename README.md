# Towards a Measure of General Intelligence
## Introduction
With the advent of deep learning, there has been a resurgence in attempts to
measure artificial intelligence in comparison to human levels. The presence of quan-
titative benchmarks in computer vision and language understanding have helped
in the steady growth of deep learning models in terms of design and performance.
Recent approaches to measure artificial intelligence focus on generalization capabil-
ity in addition to skill-based performance, and provide guidelines for the properties
of a benchmark based on psychometrics, algorithmic information theory, program
synthesis, and reinforcement learning. We propose an experimental setup where the system solves tasks by producing programs that follow a flow-based paradigm, and define a benchmark called the g-index to measure the capability of the system in this setup. Finally, we evaluate the suitability of some well-known large language models as general intelligence systems by measuring their g-index scores.

This repository contains the code to measure [g-index](#definitions) for an experiment as described in the paper [here](https://www.example.com)
## Results 

<table>
<thead>
<tr>
<th>Model Name</th>
<th>Number of <br> Parameters</th>
<th> G Index</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI GPT2 Medium</td>
<td>345M</td>
<td>1</td>
</tr>
<tr>
<td>OpenAI GPT2 Large</td>
<td>774M</td>
<td>1</td>
</tr>
<tr>
<td>OpenAI GPT2 XLarge</td>
<td>1.5B</td>
<td>1</td>
</tr>
<tr>
<td>EleutherAI GPT Neo</td>
<td>2.7B</td>
<td>1</td>
</tr>
</tbody>
</table>
<blockquote>
 Each model was trained on 8000 samples and 30 epochs 
</blockquote>

## Plots
<!-- [Replace this with a useful plot](images/sample.png "Replace this with a useful plot") -->
<img src="images/sample.png" alt="Replace this with a useful plot" style="height: 400px; width:400px;"/>

## Definitions
`intelligent_system (IS)` : the intelligent system (IS) generates a `skill program (S)` to do `task (T)` <br>
`task (T)`: a natural language prompt that forms an instruction to the machine what to do <br>
`skill program (S)`: a program or DAG generated by an `IS` to perform task `T` <br>
`curricula (C)`: data that the `IS` uses to train itself is denoted by `C` or Curricula, in our case all data in the train_array <br>
`curriculum item (C_n)` : `C_n` indicates a subset of the curriculum that is specific to a certain template/domain <br>
`domain (D)` : a related set of tasks/curricula. currently each template in our system can be it's own domain, but later we can group similar ones together into similar domains. <br>
`divergence_score (DS)` : distance between skill program created by a narrow rule-based generator, and a general-purpose `IS` <br>
`performance_theta (θ)` : performance of the `IS` on a task (`1 - DS`) <br>
`domain_distance_score` : fractional divergence/distance between any two given templates/domains/skill programs. `DS` can be used here too, if they are the same. <br>
`generalization_difficulty (GD)` : `domain_distance_score` between task domain & curricula domain (or training task and given test flow) <br>
`P(C_n)` : probability of finding curricula of domain type `C_n` in curricula. or (samples_of_template_C_n/total_number_C_samples) <br>
`priors (P)` : data which is built into the system _before_ training (are you fine-tuning something already fine-tuned?). This is negligible (~0) for most of current models. <br>
`experience (E)` : exposure of the `IS` to `Curricula` (tentative : compute * curricula?) <br>
`task_contribution` : contribution of performance on one `task (T)` to the overall `g_index` 
this is equal to, for any domain `C_n` in total n domains in curricula `C` - 
= `θ * Σ (P(C_n) * ( (GD(T,C_n)/(P + E(C_n)) )))` <br>
`tasks_scope` or `task_domains`: all the tasks/domains in the test array <br>
`g_index` :A measure of intelligence  average of task_contribution across all tasks_scope, :
`avg_across_T(θ * Σ (P(C_n) * ( (GD(T_i,C_n)/(P + E(C_n)) ))))` <br>

## Usage
### Preparation:
1. Install the Python package requirements via the following command:
   ```bash
        pip install -r requirements.txt
    ```
2. Follow the instructions [here](#request-the-data) to request the data
3. After getting the data, verify you have the following copied to the root of the repository
    1. experiments folder
    2. templates folder
    3. lengths.json file

### Using the Command Line 
The command line offers various flags to reproduce the results as claimed in the paper.

#### Options
1. `-e`, `--experiment-dir` &nbsp;[Required,&nbsp; `String`]: Set the directory where the experiment files are stored
2. `-t` `--template_dir`  &nbsp;[Required,&nbsp; `String`]: Set the directory where the template files are stored
3. `-p` `--print_results` &nbsp;[Optional,&nbsp; `Bool`]: Set whether to print the metrics on the command line.
4. `-s` `--save_metrics` &nbsp; [Optional &nbsp; `Bool`]: Set whether to dump metrics to a JSON file.

Alternatively, you can also run `python main.py -h` to see the available options
### Request the data
You can send us a mail at [humans@mayahq.com](mailto:humans@mayahq.com) breifly describing your use case to get the data.

## License

## Cite Us!
```
Citing Details
```
